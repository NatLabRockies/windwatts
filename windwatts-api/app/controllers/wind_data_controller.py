from typing import List, Optional
from fastapi import APIRouter, HTTPException, Path, Query
from fastapi.responses import StreamingResponse
import zipfile
import tempfile
import re
import os

from app.config_manager import ConfigManager
from app.config.model_config import MODEL_CONFIG, TEMPORAL_SCHEMAS
from app.data_fetchers.s3_data_fetcher import S3DataFetcher
from app.data_fetchers.athena_data_fetcher import AthenaDataFetcher
from app.data_fetchers.data_fetcher_router import DataFetcherRouter
from app.utils.data_fetcher_utils import format_coordinate, chunker
from app.utils.validation import validate_model, validate_limit
from app.utils.wind_data_core import (
    get_windspeed_core,
    get_production_core,
    get_timeseries_core,
    get_timeseries_energy_core,
)

from app.power_curve.global_power_curve_manager import power_curve_manager
from app.schemas import (
    AvailableTurbinesResponse,
    WindSpeedResponse,
    AvailablePowerCurvesResponse,
    EnergyProductionResponse,
    NearestLocationsResponse,
    TimeseriesBatchRequest,
    TimeseriesEnergyBatchRequest,
    ModelInfoResponse,
)

router = APIRouter()

# Initialize DataFetcherRouter
data_fetcher_router = DataFetcherRouter()
_skip_data_init = os.environ.get("SKIP_DATA_INIT", "0") == "1"

# Initialize data fetchers
athena_data_fetchers = {}
s3_data_fetchers = {}

if not _skip_data_init:
    # Initialize ConfigManager
    config_manager = ConfigManager(
        secret_arn_env_var="WINDWATTS_DATA_CONFIG_SECRET_ARN",
        local_config_path="./app/config/windwatts_data_config.json",
    )
    athena_config = config_manager.get_config()

    # Initialize Athena data fetchers
    athena_data_fetchers["era5-quantiles"] = AthenaDataFetcher(
        athena_config=athena_config, source_key="era5"
    )
    athena_data_fetchers["ensemble-quantiles"] = AthenaDataFetcher(
        athena_config=athena_config, source_key="ensemble"
    )
    athena_data_fetchers["wtk-timeseries"] = AthenaDataFetcher(
        athena_config=athena_config, source_key="wtk"
    )

    # Initialize S3 data fetchers
    s3_data_fetchers["era5-timeseries"] = S3DataFetcher(
        bucket_name="windwatts-era5",
        prefix="era5_timeseries",
        grid="era5",
        s3_key_template="era5",
    )
    s3_data_fetchers["wtk-timeseries"] = S3DataFetcher(
        bucket_name="wtk-led", prefix="1224", grid="wtk", s3_key_template="wtk"
    )

    # Register fetchers with DataFetcherRouter
    # Register with simple names: athena, s3 (not athena_era5, s3_era5)
    for model_key in [
        "era5-quantiles",
        "ensemble-quantiles",
        "wtk-timeseries",
        "era5-timeseries",
    ]:
        if model_key in athena_data_fetchers:
            data_fetcher_router.register_fetcher(
                f"athena_{model_key}", athena_data_fetchers[model_key]
            )
        if model_key in s3_data_fetchers:
            data_fetcher_router.register_fetcher(
                f"s3_{model_key}", s3_data_fetchers[model_key]
            )


# API Endpoints
@router.get(
    "/{model}/windspeed",
    summary="Retrieve wind speed data for a location",
    response_model=WindSpeedResponse,
    responses={
        200: {
            "description": "Wind speed data retrieved successfully",
            "model": WindSpeedResponse,
        },
        400: {"description": "Bad request - invalid parameters"},
        404: {"description": "Data not found"},
        500: {"description": "Internal server error"},
    },
)
def get_windspeed(
    model: str = Path(..., description="Data model: era5, wtk, or ensemble"),
    lat: float = Query(..., description="Latitude of the location"),
    lng: float = Query(..., description="Longitude of the location"),
    height: int = Query(..., description="Height in meters"),
    period: str = Query(
        "all", description="Time period: all, annual, monthly, hourly (varies by model)"
    ),
    source: Optional[str] = Query(
        None,
        description="Data source: athena or s3. Defaults to model's default source (athena).",
    ),
):
    """
    Retrieve wind speed data for a specific location and height.

    - **model**: Data model (era5, wtk, ensemble)
    - **lat**: Latitude (varies by model, refer info endpoint for coordinate bounds)
    - **lng**: Longitude (varies by model, refer info endpoint for coordinate bounds)
    - **height**: Height in meters (varies by model, refer info endpoint for the available heights)
    - **period**: Time aggregation period (default: all)
    - **source**: Optional data source override
    """
    try:
        # Catch invalid model before core function call
        model = validate_model(model)

        # Use default source if not provided
        if source is None:
            source = MODEL_CONFIG.get(model, {}).get("source")

        return get_windspeed_core(
            model, lat, lng, height, period, source, data_fetcher_router
        )
    except HTTPException:
        raise
    except Exception:
        raise HTTPException(status_code=500, detail="Internal server error")


@router.get(
    "/{model}/production",
    summary="Get energy production estimate for a location with a power curve",
    response_model=EnergyProductionResponse,
    response_model_exclude_none=True,
    responses={
        200: {
            "description": "Energy production data retrieved successfully",
            "model": EnergyProductionResponse,
        },
        400: {"description": "Bad request - invalid parameters"},
        404: {"description": "Data not found"},
        500: {"description": "Internal server error"},
    },
)
def get_production(
    model: str = Path(..., description="Data model: era5, wtk, or ensemble"),
    lat: float = Query(..., description="Latitude of the location"),
    lng: float = Query(..., description="Longitude of the location"),
    height: int = Query(..., description="Height in meters"),
    turbine: Optional[str] = Query(
        None, description="Turbine model identifier (e.g., nrl-reference-100kW)"
    ),
    powercurve: Optional[str] = Query(
        None,
        deprecated=True,
        description="Deprecated: use 'turbine' instead. Power curve identifier.",
    ),
    period: str = Query(
        "all",
        description="Time period: all, summary, annual, monthly (varies by model)",
    ),
    source: Optional[str] = Query(
        None,
        description="Data source: athena or s3. Defaults to model's default source (athena).",
    ),
):
    """
    Retrieve energy production estimates for a specific location, height, and turbine.

    - **model**: Data model (era5, wtk, ensemble)
    - **lat**: Latitude (varies by model, refer info endpoint for coordinate bounds)
    - **lng**: Longitude (varies by model, refer info endpoint for coordinate bounds)
    - **height**: Height in meters (varies by model, refer info endpoint for the available heights)
    - **turbine**: Turbine model to use for calculations
    - **powercurve**: Deprecated parameter, use 'turbine' instead
    - **period**: Time aggregation period (default: all)
    - **source**: Optional data source override
    """
    try:
        # Catch invalid model before core function call
        model = validate_model(model)

        # Backward compatibility for 'powercurve'
        turbine = turbine or powercurve
        if not turbine:
            raise HTTPException(
                status_code=400,
                detail="Either 'turbine' or 'powercurve' parameter is required",
            )

        # Use default source if not provided
        if source is None:
            source = MODEL_CONFIG.get(model, {}).get("source")

        return get_production_core(
            model, lat, lng, height, turbine, period, source, data_fetcher_router
        )
    except HTTPException:
        raise
    except Exception:
        raise HTTPException(status_code=500, detail="Internal server error")


def _get_available_turbines(field_name: str = "turbines"):
    """
    Retrieve all available turbines/power curves.

    Returns sorted list with NLR reference turbines first (by capacity),
    followed by other turbines alphabetically.

    Args:
        field_name: "turbines" or "power curves"
    """
    all_curves = list(power_curve_manager.power_curves.keys())

    def extract_kw(curve_name: str):
        # Extracts the kw value from nlr curves, e.g. "nlr-reference-2.5kW" -> 2.5
        match = re.search(r"nlr-reference-([0-9.]+)kW", curve_name)
        if match:
            return float(match.group(1))
        return float("inf")

    nlr_curves = [c for c in all_curves if c.startswith("nlr-reference-")]
    other_curves = [c for c in all_curves if not c.startswith("nlr-reference-")]

    nlr_curves_sorted = sorted(nlr_curves, key=extract_kw)
    other_curves_sorted = sorted(other_curves)

    ordered_curves = nlr_curves_sorted + other_curves_sorted
    return {f"available_{field_name}": ordered_curves}


@router.get(
    "/turbines",
    summary="Fetch all available turbines",
    response_model=AvailableTurbinesResponse,
    responses={
        200: {
            "description": "Available turbines retrieved successfully",
            "model": AvailableTurbinesResponse,
        },
        500: {"description": "Internal server error"},
    },
)
def get_turbines():
    """
    Retrieve a list of all available turbines.

    Turbines are model-agnostic and can be used with any dataset (era5, wtk, ensemble).
    """
    try:
        return _get_available_turbines("turbines")
    except Exception:
        raise HTTPException(status_code=500, detail="Internal server error")


@router.get(
    "/powercurves",
    summary="Fetch all available power curves",
    response_model=AvailablePowerCurvesResponse,
    deprecated=True,
    responses={
        200: {
            "description": "Available power curves retrieved successfully",
            "model": AvailablePowerCurvesResponse,
        },
        500: {"description": "Internal server error"},
    },
)
def get_powercurves():
    """
    Retrieve a list of all available power curves.

    Deprecated: Use /turbines endpoint instead.

    Power curves are model-agnostic and can be used with any dataset (era5, wtk, ensemble).
    """
    try:
        return _get_available_turbines("power_curves")
    except Exception:
        raise HTTPException(status_code=500, detail="Internal server error")


@router.get(
    "/{model}/grid-points",
    summary="Find nearest grid locations to a coordinate",
    response_model=NearestLocationsResponse,
    responses={
        200: {"description": "Nearest grid locations retrieved successfully"},
        400: {"description": "Bad request - invalid parameters"},
        500: {"description": "Internal server error"},
    },
)
def get_grid_points(
    model: str = Path(..., description="Data model: era5, wtk, or ensemble"),
    lat: float = Query(..., description="Latitude of the target location"),
    lng: float = Query(..., description="Longitude of the target location"),
    limit: int = Query(1, description="Number of nearest grid points to return (1-4)"),
):
    """
    Find the nearest grid points to a given coordinate.

    Returns grid indices and their coordinates for the closest data points in the model's grid.

    - **model**: Data model (era5, wtk, ensemble)
    - **lat**: (varies by model, refer info endpoint for coordinate bounds)
    - **lng**: (varies by model, refer info endpoint for coordinate bounds)
    - **limit**: Number of nearest points to return (1-4)
    """
    try:
        model = validate_model(model)

        # Grid lookup only available via athena
        # Use athena fetcher for the specified model
        fetcher = athena_data_fetchers.get(model)

        if not fetcher or not hasattr(fetcher, "find_nearest_locations"):
            raise HTTPException(
                status_code=400,
                detail=f"Grid point lookup not available for model '{model}'",
            )

        # Call find_nearest_locations on the fetcher
        limit = validate_limit(limit)
        result = fetcher.find_nearest_locations(lat=lat, lng=lng, n_neighbors=limit)

        locations = [
            {"index": str(i), "latitude": float(a), "longitude": float(o)}
            for i, a, o in result
        ]

        return {"locations": locations}
    except HTTPException:
        raise
    except Exception:
        raise HTTPException(status_code=500, detail="Internal server error")


@router.get(
    "/{model}/info",
    summary="Get metadata information about a data model",
    response_model=ModelInfoResponse,
    responses={
        200: {"description": "Model information retrieved successfully"},
        400: {"description": "Invalid model"},
        500: {"description": "Internal server error"},
    },
)
def get_model_info(
    model: str = Path(..., description="Data model: era5, wtk, or ensemble"),
):
    """
    Retrieve metadata and configuration information about a specific data model.

    Returns information about:
    - Supported time periods
    - Available years for timeseries downloads
    - Available heights
    - Grid information (modelâ€™s geographic coverage and spatial-temporal resolution)
    - Links & References

    - **model**: Data model (era5, wtk, ensemble)
    """
    try:
        model = validate_model(model)
        config = MODEL_CONFIG[model]
        schema = config["schema"]
        temporal_config = TEMPORAL_SCHEMAS[schema]
        return {
            "model": model,
            # "available_sources": config["sources"],
            # "default_source": config["default_source"],
            "supported_periods": temporal_config["period_type"],
            "available_years": config.get("years", {}).get("full", []),
            "sample_years": config.get("years", {}).get("sample", []),
            "available_heights": config.get("heights", []),
            "grid_info": config.get("grid_info", {}),
            "links": config.get("links", []),
            "references": config.get("references", []),
        }
    except HTTPException:
        raise
    except Exception:
        raise HTTPException(status_code=500, detail="Internal server error")


@router.get(
    "/{model}/timeseries",
    summary="Download timeseries CSV data for a specific grid point",
    responses={
        200: {"description": "CSV file downloaded successfully"},
        400: {"description": "Bad request - invalid parameters"},
        404: {"description": "Data not found"},
        500: {"description": "Internal server error"},
    },
)
def download_timeseries(
    model: str = Path(..., description="Data model: era5 or wtk"),
    gridIndex: str = Query(..., description="Grid index identifier"),
    year_range: Optional[str] = Query(
        None, description="Range of years for download. Format: YYYY-YYYY"
    ),
    year_set: Optional[str] = Query(
        None, description="Download full or sample dataset"
    ),
    years: Optional[List[int]] = Query(
        None, description="Years to download (defaults to sample years)"
    ),
    period: Optional[str] = Query(
        "hourly",
        description="Time aggregation: hourly (raw data) or monthly (yyyy-mm aggregation)",
    ),
    source: str = Query(
        "s3",
        description="Data source: athena or s3 (typically s3 for timeseries downloads)",
    ),
):
    """
    Download timeseries data as CSV for a specific grid point.

    Returns raw timeseries data for the specified grid index and years.

    - **model**: Data model (era5, wtk)
    - **gridIndex**: Grid index from grid-points endpoint
    - **year_range**: Range of years for download. Format: YYYY-YYYY.
    - **year_set**: Full or Sample dataset to download (optional)
    - **years**: List of years to include (optional)
    - **period**: Time aggregation (hourly for raw data, monthly for yyyy-mm grouped averages)
    - **source**: Data source (defaults to S3)
    """
    try:
        # Get CSV content from core function
        csv_content = get_timeseries_core(
            model,
            [gridIndex],
            period,
            source,
            data_fetcher_router,
            years,
            year_range,
            year_set,
        )

        return StreamingResponse(
            iter([csv_content]),
            media_type="text/csv; charset=utf-8",
            headers={
                "Content-Disposition": f'attachment; filename="wind_data_{gridIndex}.csv"'
            },
        )

    except HTTPException:
        raise
    except Exception:
        raise HTTPException(status_code=500, detail="Internal server error")


@router.post(
    "/{model}/timeseries/batch",
    summary="Download multiple timeseries CSVs as a ZIP file",
    responses={
        200: {"description": "ZIP file downloaded successfully"},
        400: {"description": "Bad request - invalid parameters"},
        404: {"description": "Data not found"},
        500: {"description": "Internal server error"},
    },
)
def download_timeseries_batch(
    payload: TimeseriesBatchRequest,
    model: str = Path(..., description="Data model: era5 or wtk"),
):
    """
    Download timeseries data for multiple grid points as a ZIP archive.

    Accepts a request body with grid locations, optional years, and data source.
    Returns a ZIP file containing CSV files for each location.

    - **model**: Data model (era5, wtk)
    - **payload**: Request body containing:
      - **locations**: List of grid locations with indices (use grid-points endpoint)
      - **years**: List of years to include (optional, defaults to sample years)
      - **year_range**: Range of years for download. Format: YYYY-YYYY. (optional)
      - **year_set**: Full or Sample dataset to download (optional)
      - **source**: Data source (optional, defaults to s3)
      - **period**: Time aggregation (hourly for raw data, monthly for yyyy-mm grouped averages)
    """
    try:
        # Create spooled temporary file for ZIP
        spooled = tempfile.SpooledTemporaryFile(max_size=30 * 1024 * 1024, mode="w+b")

        with zipfile.ZipFile(spooled, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
            for loc in payload.locations:
                csv_content = get_timeseries_core(
                    model,
                    [loc.index],
                    payload.period,
                    payload.source,
                    data_fetcher_router,
                    payload.years,
                    payload.year_range,
                    payload.year_set,
                )
                file_name = f"wind_data_{format_coordinate(loc.latitude)}_{format_coordinate(loc.longitude)}.csv"
                zf.writestr(file_name, csv_content)

        spooled.seek(0)

        headers = {
            "Content-Disposition": f'attachment; filename="wind_data_{model}_{len(payload.locations)}_points.zip"'
        }

        return StreamingResponse(
            chunker(spooled), media_type="application/zip", headers=headers
        )

    except HTTPException:
        raise
    except Exception:
        raise HTTPException(status_code=500, detail="Internal server error")


@router.get(
    "/{model}/timeseries/energy",
    summary="Download timeseries CSV data along with energy estimates for a selected turbine.",
    responses={
        200: {"description": "CSV file downloaded successfully"},
        400: {"description": "Bad request - invalid parameters"},
        404: {"description": "Data not found"},
        500: {"description": "Internal server error"},
    },
)
def download_energy_timeseries(
    model: str = Path(..., description="Data model: era5 or wtk"),
    gridIndex: str = Query(..., description="Grid index indentifier"),
    turbine: str = Query(..., description="Turbine for energy estimates"),
    year_range: Optional[str] = Query(
        None, description="Range of years for download. Format: YYYY-YYYY"
    ),
    year_set: Optional[str] = Query(
        None, description="Download full or sample dataset"
    ),
    years: Optional[List[int]] = Query(
        None, description="Years to download (default to sample years)"
    ),
    period: Optional[str] = Query(
        "hourly",
        description="Time aggregation: hourly (raw data) or monthly (yyyy-mm aggregation)",
    ),
    source: str = Query(
        "s3",
        description="Data source: athena or s3 (typically s3 for timeseries downloads)",
    ),
):
    """
    Download energy timeseries data as CSV for a specific grid point.

    - **model**: Data model (era5, wtk)
    - **gridIndex**: Grid index from grid-points endpoint
    - **turbine**: Turbine model to use for energy calculations
    - **year_range**: Range of years for download. Format: YYYY-YYYY. (optional)
    - **year_set**: Full or Sample dataset to download (optional)
    - **years**: List of years to include (optional)
    - **period**: Time aggregation (hourly for raw data, monthly for yyyy-mm grouped averages)
    - **source**: Data source (defaults to S3)
    """
    try:
        csv_content = get_timeseries_energy_core(
            model,
            [gridIndex],
            turbine,
            period,
            source,
            data_fetcher_router,
            years,
            year_range,
            year_set,
        )

        return StreamingResponse(
            iter([csv_content]),
            media_type="text/csv; charset=utf-8",
            headers={
                "Content-Disposition": f'attachment; filename="wind_&_energy_data_{gridIndex}.csv"'
            },
        )
    except HTTPException:
        raise
    except Exception:
        raise HTTPException(status_code=500, detail="Internal server error")

@router.post(
    "/{model}/timeseries/energy/batch",
    summary="Download multiple timeseries CSV data along with energy estimates for a selected turbine as a ZIP file.",
    responses={
        200: {"description": "ZIP file downloaded successfully"},
        400: {"description": "Bad request - invalid parameters"},
        404: {"description": "Data not found"},
        500: {"description": "Internal server error"},
    },
)
def download_timeseries_energy_batch(
    payload: TimeseriesEnergyBatchRequest,
    model: str = Path(..., description="Data model: era5 or wtk"),
):
    """
    Download timeseries data for multiple grid points as a ZIP archive.

    Accepts a request body with grid locations, optional years, and data source.
    Returns a ZIP file containing CSV files for each location.

    - **model**: Data model (era5, wtk)
    - **payload**: Request body containing:
      - **locations**: List of grid locations with indices (use grid-points endpoint)
      - **turbine**: Turbine model to use for energy calculations
      - **years**: List of years to include (optional, defaults to sample years)
      - **year_range**: Range of years for download. Format: YYYY-YYYY. (optional)
      - **year_set**: Full or Sample dataset to download (optional)
      - **source**: Data source (optional, defaults to s3)
      - **period**: Time aggregation (hourly for raw data, monthly for yyyy-mm grouped averages)
    """
    try:
        # Create spooled temporary file for ZIP
        spooled = tempfile.SpooledTemporaryFile(max_size=30 * 1024 * 1024, mode="w+b")

        with zipfile.ZipFile(spooled, mode="w", compression=zipfile.ZIP_DEFLATED) as zf:
            for loc in payload.locations:
                csv_content = get_timeseries_energy_core(
                    model,
                    [loc.index],
                    payload.turbine,
                    payload.period,
                    payload.source,
                    data_fetcher_router,
                    payload.years,
                    payload.year_range,
                    payload.year_set,
                )
                file_name = f"wind_&_energy_data_{format_coordinate(loc.latitude)}_{format_coordinate(loc.longitude)}.csv"
                zf.writestr(file_name, csv_content)

        spooled.seek(0)

        headers = {
            "Content-Disposition": f'attachment; filename="wind_&_energy_data_{model}_{len(payload.locations)}_points.zip"'
        }

        return StreamingResponse(
            chunker(spooled), media_type="application/zip", headers=headers
        )

    except HTTPException:
        raise
    except Exception:
        raise HTTPException(status_code=500, detail="Internal server error")
